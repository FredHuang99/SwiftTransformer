# SwiftTransformer

SwiftTransformer is a tiny yet powerful and flexible implementation of the transformer neural network. It aims at providing a framework for researchers to try on their novel ideas.

It has the following advantages:

- **Tiny.** It only contains essential code for running the GPT model, thus you can get your hands on it without much effort and implement your world-changing ideas.
- **Efficient.** We leverage custom CUDA kernels to accelerate the inference process.
- **Flexible.** It is easy to extend the model with your own ideas. For example, changing the order of computation, or modifying how the key/value cache is stored, should not be a problem.
- **Well-documented.** Compared to [NVIDIA's FasterTransformer](https://github.com/NVIDIA/FasterTransformer/tree/main#global-environment), we provide much more detailed documentation and examples.

This repo acts as the computation backend (i.e. data path) in our serving systems, including [DistServe](https://github.com/LLMServe/DistServe) and [FastServe](https://github.com/LLMServe/FastServe)'s

## Build

Please refer to [DistServe](https://github.com/LLMServe/DistServe) or [FastServe](https://github.com/LLMServe/FastServe)'s README about how to build SwiftTransformer.

## Run

NOTE. Users are expected to use high-level serving frameworks based on SwiftTransformer (including [DistServe](https://github.com/LLMServe/DistServe) and [FastServe](https://github.com/LLMServe/FastServe)'s), instead of using this repo directly.

We provide an example to run the model on OPT weights. To run the example, please:

- **Download the vocab.** Download `gpt2-merges.txt` and `gpt2-vocab.json` from [Here](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/assets). For example, 
  ```shell
  mkdir models
  wget https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT/assets/gpt2-merges.txt -O models/gpt2-merges.txt
  wget https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT/assets/gpt2-vocab.json -O models/gpt2-vocab.json
  ```
- **Download the model.** Select your favourite model from [Here](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT) and download the weights. For example, 
  ```shell
  wget https://dl.fbaipublicfiles.com/opt/v1_20230405/1.3b/reshard-model_part-0.pt -O models/opt-1.3b.pt
  ```
  Note: Please do not choose OPT-350M since its architecture is different from other models.
- **Convert the model.** The weight file is stored in .pt format (generated by `torch.save()`), which cannot be loaded by LibTorch, so we need to convert it.

  Use `python3 scripts/convert-opt.py --input <path/to/your/downloaded/model> --output <path/to/converted/weights> --dtype <datatype (fp16 or fp32)>`

  For example, 
  ```shell
  python3 scripts/convert-opt.py --input models/opt-1.3b.pt --output models/opt-1.3b-conv-fp16.pt --dtype fp16
  ```
- **Prepare your input.**. Use `python3 scripts/encode_input.py <path/to/vocab.json> <path/to/merges.txt>` to encode your input. This script accepts your requests from stdin (one per line) and outputs the encoded input to stdout. For example, 
  ```shell
  mkdir inputs
  printf "Life blooms like a flower. Far away or by the road. Waiting for the one, to\nA quick brown fox\nArtificial intelligence is\nTo be or not to be," > inputs/input1_plain.txt
  python3 scripts/encode_input.py models/gpt2-vocab.json models/gpt2-merges.txt < inputs/input1_plain.txt > inputs/input1_encoded.txt
  ```
- **Run the model.** Use `build/bin/run_opt` to run the model. For example, 
  ```shell
  build/bin/run_opt models/opt-1.3b-conv-fp16.pt 1.3b models/gpt2-vocab.json fp16 inputs/input1_encoded.txt
  ```

## Testing

We provide various unit tests to test the correctness of components of the model. To run the test, please compile the project, and then execute `bin/unittest_XXX` in the `build` directory.

## Development

### Code Structure

Currently, the code is organized as follows:

```text
src
├── csrc
│   ├── kernel
│   ├── layer
│   ├── model
│   ├── pybinding.cc
│   └── util
├── examples
│   ├── benchmark_all_input_same.cc
│   ├── CMakeLists.txt
│   ├── lib
│   └── run_gpt.cc
└── unittest
    ├── kernel
    ├── layer
    ├── model
    ├── unittest_torch_utils.h
    ├── unittest_utils.h
    └── util
```

The `csrc` folder contains the core implementation of the model, including every kernel, layer and model.

The `unittest` folder contains unit tests for the components in `csrc`. The `kernel`, `layer`, `model`, and `util` folders under the `unittest` folder contain the implementation of the corresponding components. For example, `src/unittest/layer/attention.cc` contains the unit test for the `Attention` layer, which is implemented in `src/csrc/layer/attention.cc`.

Note for vscode users: If you encounter `#include errors detected. Please update your includePath.`, you may need to update include path in `.vscode/c_cpp_properties.json`.

### Design Philosophy

- **Well-documented.** We strongly believe that a well-documented codebase boosts the efficiency of research. Therefore we try our best to document every function and class. **Typically we explain the purpose and meanings of arguments of a function before its implementation in the `.cc` file.**
- **POP-styled design.** Different from FastTransformer which adopts an Object-oriented programming (OOP) design, we adopt a more Procedure-Oriented Programming (POP) style. We believe that POP is more suitable for research projects, since it is easier to extend and modify the code. Think why we need OOP, and you will find the answer is "to hide the details". However in research projects, we need to know, and alter the details. Therefore all kernels and layers are implemented in POP style.
- **Extensive unit tests.** Every kernel and layer is paired with a unit test. We believe that unit tests are essential for research projects, since they can help us to verify the correctness of our implementation. We use [googletest](https://github.com/google/googletest) as our unit test framework. With the help of `TYPED_TEST` from googletest, we can test our kernels and layers with different data types (e.g. `float` and `half`) without writing redundant code.
- **LibTorch for reference in unit tests.** For the "reference" part in unittests, we use LibTorch to implement the same kernel or layer. This is because LibTorch is well-tested, and we can use it as a reference to verify the correctness of our implementation.
- **Raw pointers instead of `at::Tensor`.** We prefer the raw pointer in C over `at::Tensor` (The tensor class provided by LibTorch, the C++ frontend of PyTorch), since we need fine-grained control over the memory layout.

### Prerequisite Knowledge

Time for you to get your hands on! Here are some tutorials to help you get started:

- CMake: You need to know what `target` is and what `target_link_libraries` does. Here is a tutorial: [An Introduction to Modern CMake
](https://cliutils.gitlab.io/modern-cmake/)
- GoogleTest: To write unit tests, a brief look at googletest's manual is necessary: [The googletest primer](https://google.github.io/googletest/primer.html)
- CUDA: Interested in CUDA? Here are some tutorials:
  - For beginners: [An Even Easier Introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/), [CUDA Tutorial](https://www.tutorialspoint.com/cuda/index.htm).
  - Performance insight and optimization: [GPU Performance Background User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html), [CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)
  - Interesting stuff: [A HISTORY OF NVIDIA STREAM MULTIPROCESSOR](https://fabiensanglard.net/cuda/)